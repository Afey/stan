\documentclass[article]{jss}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% almost as usual
\author{{\bf\large Bob Carpenter}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Daniel Lee}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Marcus Brubaker}
        \\ Toyota Technological Institute
        %
    \And
        %
        {\bf\large Andrew Gelman}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Ben Goodrich}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Jiqiang Guo}
        \\ Columbia Univesity
        %
     \And
        %
        {\bf\large Matt Hoffman}
        \\ Adobe Research Labs
        %
        \\[9pt]
        %
        {\bf\large Michael Betancourt}
        \\ University of Warwick
        %
        \\[9pt]
        %
        {\bf\large Peter Li}
        \\ Columbia University
}
\title{\proglang{Stan}: A Probabilistic Programming Language}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Bob Carpenter, Andrew Gelman, Matt Hoffman, 
  Daniel Lee, Ben Goodrich, Michael Betancourt, 
  Marcus Brubaker, Jiqiang Guo, Peter Li} %% comma-separated
\Plaintitle{Stan: A Probabilistic Programming Language} %% without formatting
\Shorttitle{Stan: A Probabilistic Programming Language} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ \proglang{Stan} is a probabilistic programming language for
  specifying statistical models. A \proglang{Stan} program
  imperatively defines a log probability function over parameters
  conditioned on specified data and constants.  This may be contrasted
  with the \proglang{BUGS} language, in which a program declaratively
  defines a directed acyclic graphical model.

  Variables are declared for dimensionality and as to whether they
  represent data, transformed data, parameters, transformed
  parameters, or generated quantities; local variables are also
  supported.  There are unconstrained and constrained scalar,
  discrete, vector, matrix, and array types.  There is a library of
  mathematical functions, probability-related functions, and matrix
  and linear algebra functions.  Statements, such as assignment,
  sampling, conditionals, and loops are executed imperatively in the
  order they are written.

  Full Bayesian inference is supported through Markov chain Monte
  Carlo (MCMC) methods such as the no-U-turn Hamiltonian Monte
  Carlo sampler (NUTS).  Penalized maximum likelihood estimates
  are calculated using optimization methods such as BFGS.
  
  \proglang{Stan} programs are translated to \proglang{C++} and
  compiled.  The target \proglang{C++} code includes forward- and
  reverse-mode algorithmic differentiation to support samplers and
  optimizers requiring gradients, Hessians, Hessian-vector products,
  and other higher-order derivatives.  There is an extensive I/O
  library to deal with constrained variable input for data,
  initialization for parameters, and output for samples or point
  estimates.  There are also a range of tools to monitor convergence
  and mixing, summarize the posterior, perform Bayesian inference, and
  carry out posterior predictive checks.

  \proglang{Stan} can be called from the command line, through
  \proglang{R} using the \pkg{RStan} package, or through
  \proglang{Python} using the \pkg{PyStan} package.  All three
  interfaces support sampling or optimization-based inference and
  analysis.}

\Keywords{
  probabilistic programming,
  Bayesian inference,
  algorithmic differentiation,
  \proglang{Stan}}
%
\Plainkeywords{
  probabilistic programming,
  Bayesian inference,
  algorithmic differentiation,
  Stan}


%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Bob Carpenter
  \\
  Department of Statistics
  \\ 
  Columbia University
  \\ 
  1255 Amsterdam Avenue
  \\ 
  New York, NY 10027
  \\
  U.S.A.
  \\
  E-mail: \email{carp@stat.columbia.edu}
  \\ 
  URL: \url{http://mc-stan.org/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Why Stan?]{Why \proglang{Stan}?}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

We did not set out to build \proglang{Stan} as it currently exists.
We set out to apply full Bayesian inference to the sort of multilevel
generalized linear models discussed in Part~II of
\citep{GelmanHill:2007}.  These models are structured with grouped and
interacted predictors at multiple levels, hierarchical covariance
priors, nonconjugate coefficient priors, latent effects as in
item-response models, and varying output link functions and
distributions.

The models we wanted to fit turned out to be a challenge for existing
general-purpose software to fit.  A direct encoding in \proglang{BUGS}
or \proglang{JAGS} can grind these tools to a halt.  We added custom
vectorized logistic regressions to \proglang{JAGS} using
\proglang{C++} to overcome the cost of interpretation.  Although this
is much faster than looping in \proglang{JAGS}, the root of the
problem was that conditional sampling took to long to converge when
parameters were highly correlated in the posterior, as for time-series
models and hierarchical models with interacted predictors.  We finally
realized we needed a better sampler, not a more efficient
implementation.

We briefly considered trying to tune proposals for a random-walk
Metropolis-Hastings sampler, but that seemed too problem specific and
not even necessarily possible without some kind of adaptation rather
than tuning of the proposals. 

We were hearing more about HMC (Hamiltonian Monte Carlo), which
appeared promising but was also problematic in that the Hamiltonian
dynamics simulation requires the gradient of the log posterior.
Although it is possible to do this by hand on a model-by-model basis,
it is very tedious and error prone.  That is when we discovered
reverse-mode algorithmic differentiation, which lets you write down a
templated \proglang{C++} function for the log posterior and
automatically compute a proper analytic gradient up to machine
precision accuracy in only a few multiples of the cost to evaluate the
log probability function itself.  We explored existing algorithmic
differentiation packages with open licenses such as {\sc rad}
\citep{Gay:2005} and its repackaging in the Sacado module of the
Trilinos toolkit and the {\small CppAD} package in the {\sc coin-or}
toolkit.  But neither package supported very many special functions
(e.g., probability functions, log gamma, inverse logit) or linear
algebra operations (e.g., Cholesky decomposition) and were not easily
and modularly extensible.

So we built our own reverse-mode algorithmic differentiation package.
At that point, we ran into the problem problem that we could not just
plug in the probability functions from a package like \pkg{Boost} because
they weren't templated generally enough across all the arguments.
Rather than pay the price of promoting floating point values to
automatic differentiation variables, we wrote our own fully templated
probability functions and other special functions.

Next, we integrated the \proglang{C++} package \pkg{Eigen} for matrix
operations and linear algebra functions.  \pkg{Eigen} makes extensive use of
expression templates for lazy evaluation and the CRTP (curiously
recurring template pattern) to implement concepts without virtual
function calls.  But we ran into the same problem with Eigen as with
the existing probability libraries --- it doesn't support mixed
operations of algorithmic differentiation variables and primitives
like \code{double}.  Although we initially begain by promoting
floating-point vectors to automatic differentiation variables, we
later completely rewrote our matrix library to efficiently compute
derivatives using rules described in \citep{Giles:??,book:??}.

At this point, we could fit models coded directly in \proglang{C++} on
top of the pre-release versions of the \proglang{Stan} API
(application programming interface).  Seeing how well this all worked,
we set our sights on the generality and ease of use of
\proglang{BUGS}.  So we designed a modeling language in which
statisticians could write their models in familiar notation that could
be transformed to efficient \proglang{C++} code and then compiled into
an efficient executable program.  This paper is primarily about this
language and how it describes models.
 
The next problem we ran into as we started implementing richer models
is variables with constrained support, such as simplexes and
covariance matrices.  Although it is possible to implement HMC with
bouncing for simple boundary constraints (e.g., positive scale or
precision parameters), it is not so easy with more complex
multivariate constraints.  To get around this problem, we introduced
typed variables and automatically transformed them to unconstrained
support with suitable adjustments to the log probability from the log
absolute Jacobian determinant of the inverse transforms.

Even with the prototype compiler generating models, we still faced a
major hurdle for ease of use. The efficiency of HMC is very sensitive
to two tuning parameters, discretization interval and a total
simulation time (equivalently for simple HMC, step size and number of
steps).  The step size parameter could be tuned during warm-up based
on Metropolis rejection rates, but the number of steps was not so easy
to tune while maintaining detailed balance in the sampler.  This led
to the development of the NUTS (no U-turn) sampler
\citep{Hoffman-Gelman:2012}.  Roughly speaking, NUTS extends the
trajectory along which the Metropolis proposal for the next state is
calculated randomly in both direction until the path makes a U-turn
and begins retracing its steps.

We thought we were home free at this point.  But when we measured the
speed of some \proglang{BUGS} examples versus \proglang{Stan}, we were
very disappointed.  \proglang{BUGS}'s very first example model, Rats,
ran more than an order of magnitude faster in \proglang{JAGS} than in
\proglang{Stan}.  Rats is a tough test case because the conjugate
priors and lack of posterior correlations make it an ideal candidate
for efficient Gibbs sampling.  

We realized we were doing redundant calculations, so we wrote a
vectorized form of the normal distribution for multiple variates with
the same mean and scale, which sped things up a bit. At the same time,
we introduced some simple template metaprograms to remove the
calculation of constant terms in the log probability.  This also
improved speed, but not enough.  Finally, we figured out how to both
vectorize and partially evaluate the gradients of the densities using
a combination of expression templates and metaprogramming. 

Later, when we were trying to fit a time-series model, we found that
normalizing the data to unit sample mean and variance sped up the fits
by an order of magnitude.  Although basic HMC and NUTS are rotation
invariant (explaining why they can sample effectively from
multivariate densities with high correlations), they are not scale
invariant.  Gibbs sampling, on the other hand, is scale invariant, but
not rotation invariant.

We were still using a unit mass matrix in the simulated Hamiltonian
dynamics.  The last tweak to \proglang{Stan} before version 1.0 was to estimate
a diagonal mass matrix during warmup; this has since been upgraded to
a full mass matrix in \proglang{Stan} version 1.2.  Using a mass
matrix sped up the unscaled data models by an order of magnitude,
though it breaks the nice theoretical property of rotation invariance.
The full mass matrix estimation has rotational invariance as well, but
scales less well because of the need to invert the mass matrix once
and then do matrix multiplications every leapfrog step.

\section{Overview}

This section describes a simple use of \proglang{Stan} from the
command line to estimate a very simple Bernoulli model.

\subsection{Model for estimating a Bernoulli parameter}

Consider estimating the chance of success parameter for a Bernoulli
distribution based on a sequence of observed binary outcomes.  
Figure~\ref{bernoulli-model.fig} provides an implementation of such a
model in \proglang{Stan}.
%
\begin{figure}
\begin{Code}
data { 
  int<lower=0> N; 
  int<lower=0,upper=1> y[N];
} 
parameters {
  real<lower=0,upper=1> theta;
} 
model {
  theta ~ beta(1,1);  // prior
  for (n in 1:N) 
    y[n] ~ bernoulli(theta);  // likelihood
}
\end{Code}
\caption{Model for estimating a Bernoulli parameter.}\label{bernoulli-model.fig}
\end{figure}
%
The model assumes the binary observed data \code{y[1],...,y[N]} are
i.i.d.\ with chance of success \code{theta}.  The prior on
\code{theta} is \code{beta(1,1)} (i.e., uniform).  This model is
available in the distribution in file \code{src/models/basic\_estimators/bernoulli.stan}.


\subsection{Data format}

Data for running \proglang{Stan} from the command line can be included
in \proglang{R} dump format.  For example, 10 observations for the
model in Figure~\ref{bernoulli-model.fig} could be encoded as follows.
%
\begin{Code}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Code}
%
This defines the contents of two variables, an integer \code{N} and a
10-element integer array \code{y}.  In \pkg{RStan}, data can be passed
directly through memory without the need to write to a file.  This
data file is provided with the distrbution in file
\code{src/models/basic\_estimators/bernoulli.R.stan}.

\subsection{Compling the model}

After a \proglang{C++} compiler and \code{make} are installed,%
%
\footnote{Appropriate versions are built into Linux. The \pkg{RTools}
  package suffices for Windows and \pkg{Xcode} for the Mac.}
%
the Bernoulli model in Figure~\ref{bernoulli-model.fig} can be
translated to \proglang{C++} and compiled with a single command.
First, the directory must be changed to \code{$stan}, which we use as
a shorthand for the directory in which \proglang{Stan} is installed.%
%
\footnote{Before the first model is built, \code{make} must build the
  model translator (\code{bin/stanc}) and the \proglang{C++} library
  (\code{bin/libstan.a}) with high levels of optimization, so please
  be patient (and consider \code{make} option \code{-j2} or \code{-j4}
  to run in the specified number of processes if two, four, or more
  computational cores are available).}
%
\begin{CodeChunk}
\begin{CodeInput}
> cd $stan
> make src/models/basic_estimators/bernoulli 
\end{CodeInput}
\end{CodeChunk}
%
This produces an executable file \code{bernoulli}
(\code{bernoulli.exe} on Windows) on the specified path; forward
slashes can be used with \code{make} on Windows).

\subsection{Running the model}

\subsubsection{Command to run the model}

The executable can be run as specified given a path to the data file
(here \code{bernoulli.data.R} in the current directory; for
Windows, the \code{./} before the command is not necessary).
%
\begin{CodeChunk}
\begin{CodeInput}
> cd $stan/src/models/basic_estimators
./bernoulli --data=bernoulli.data.R --samples=samp1.csv --chain_id=1 --seed=7386
\end{CodeInput}
\end{CodeChunk}
%
This invocation specifies the executable program, the data file, the
file to which the samples are written, the identity of the chain, and
a seed for the random number generator.

\subsubsection{Terminal output}

The output is as follows, starting with a summary of the command-line
options used, including defaults;  these are also written into the 
samples file as comments.
%
\begin{Code}
STAN SAMPLING COMMAND
data = bernoulli.data.R
init = random initialization
init tries = 1
samples = samp1.csv
append_samples = 0
save_warmup = 0
seed = 7386 (user specified)
chain_id = 1 (user specified)
iter = 2000
warmup = 1000
thin = 1
equal_step_sizes = 0
nondiag_mass = 0
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
delta = 0.5
gamma = 0.05
Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:   10 / 2000 [  0%]  (Warmup)
...
Iteration: 1990 / 2000 [ 99%]  (Sampling)
Iteration: 2000 / 2000 [100%]  (Sampling)

# Elapsed Time: 0.011269 seconds (Warm-up)
#               0.014105 seconds (Sampling)
#               0.025374 seconds (Total)
\end{Code}
%
A description of all parameters is available with the \code{--help}
option; they provide control over input, output, and the configuration
of the sampler.

\subsubsection{Samples file output}

The output CSV file starts with a summary of the parameters for the
run.  
%
\begin{Code}
# Samples Generated by Stan
#
# stan_version_major=1
# stan_version_minor=3
# stan_version_patch=0
# model=bernoulli_model
# data=bernoulli.data.R
# init=random initialization
# append_samples=0
# save_warmup=0
# seed=7386
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# nondiag_mass=0
# equal_step_sizes=0
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
# algorithm=NUTS with a diagonal Euclidean metric
\end{Code}
%
It continues with a header indicating the columns of the output.
%
\begin{Code}
lp__,accept_stat__,stepsize__,treedepth__,theta
\end{Code}
%
The values for \code{lp\_\_} indicate the log probability (up to an
additive constant), the Metropolis acceptance statistic (which is
complicated by the NUTS sampler), the tree depth for NUTS (log base 2
of number of leapfrog steps in HMC), and step sizes and inverse mass
matrix for HMC.  The rest of the header will be the names of parameters.

Next, the results of adaptation are printed.
%
\begin{Code}
# Adaptation terminated
# Step size = 2.10898
# Diagonal elements of inverse mass matrix:
# 0.465584
\end{Code}
%
By default, the execution above uses a diagonal mass matrix, which
with one parameter is a single element.

The rest of the file contains samples, one per line, matching the
header; here the parameter \code{theta} is the final value printed on
each line, and each line corresponds to a sample.  The warmup samples
are not included by default, but may be included with the appropriate
command-line invocation of the executable.
%
\begin{Code}
-7.04574,0.189209,2.10898,0,0.353589
-7.04574,0.00162111,2.10898,0,0.353589
-6.77448,1,2.10898,0,0.222005
...
-7.89094,0.346733,2.10898,0,0.46154
-6.93412,1,2.10898,0,0.330879
-6.93412,0.115548,2.10898,0,0.330879

# Elapsed Time: 0.011269 seconds (Warm-up)
#               0.014105 seconds (Sampling)
#               0.025374 seconds (Total)
\end{Code}
%

%
There is one multiplier per parameter, and they are standardized to
identify theproduct of step size and parameter size used for sampling.
Because of Metropolis rejection, parameter values may be repeated.
The file ends with comments reporting the elapsed time.

\subsection{Output analysis}

\proglang{Stan} supplies a command-line program \code{bin/print} to
summarize the output of one or more MCMC chains.  Convergence analysis
includes conservative $\hat{R}$ convergence monitoring statistics 
\citep{GelmanRubin:1992, GelmanEtAl:2013}
conservative effective sample size calculations on a per-parameter
basis calculated using a combination of within-chain and across-chain
statistics.  Further analysis includes posterior quantiles, means and
and standard deviations per parameters, along with MCMC error
estimates for the means.

Three more chains of samples can be created as follows.%
%
\footnote{These four commands can be safely run in parallel under
  different processes.  How to do this depends on the operating system
  and the shell or terminal program.}
%
\begin{CodeChunk}
\begin{CodeInput}
> ./bernoulli --data=bernoulli.data.R --samples=samp2.csv --chain_id=2 --seed=7386
> ./bernoulli --data=bernoulli.data.R --samples=samp3.csv --chain_id=3 --seed=7386
> ./bernoulli --data=bernoulli.data.R --samples=samp4.csv --chain_id=4 --seed=7386
\end{CodeChunk}
%
Note that the same seed is used for each chain;  the chain identifier
is used to skip the random number generator ahead so that the (pseudo)
random numbers generated for each chain are independent.
%
\begin{CodeChunk}
\begin{CodeInput}
> ls samp*.csv
\end{CodeInput}
\begin{CodeOutput}
samp1.csv	samp2.csv	samp3.csv	samp4.csv
\end{CodeOutput}
\end{CodeChunk}
%
Then the \code{bin/print} command can be used to output summary
statistics for the posterior along with convergence diagnostics and
effective sample-size estimates.
%
\begin{CodeChunk}
\begin{CodeInput}
> ../../../bin/print samp*.csv
\end{CodeInput}
\small
\begin{CodeOutput}
Inference for Stan model: bernoulli_model
4 chains: each with iter=(1000,1000,1000,1000); warmup=(0,0,0,0); thin=(1,1,1,1); 
4000 iterations saved.

Warmup took (0.012, 0.010, 0.011, 0.011) seconds, 0.046 seconds total
Sampling took (0.016, 0.014, 0.014, 0.015) seconds, 0.060 seconds total

                   mean   se   sd     2.5%      50%    97.5%   n_eff n_eff/s    Rhat
lp__           -7.3e+00  0.0  0.7 -9.3e+00 -7.0e+00 -6.7e+00 1.1e+03 1.9e+04 1.0e+00
accept_stat__   5.3e-01  0.0  0.4  1.1e-06  5.2e-01  1.0e+00 3.7e+03 6.1e+04 1.0e+00
stepsize__      2.1e+00  0.0  0.0  2.0e+00  2.1e+00  2.1e+00 2.0e+00 3.3e+01 1.3e+13
treedepth__     3.6e-02  0.0  0.2  0.0e+00  0.0e+00  1.0e+00 2.9e+03 4.9e+04 1.0e+00
theta           2.5e-01  0.0  0.1  6.0e-02  2.4e-01  4.9e-01 2.1e+03 3.5e+04 1.0e+00

Samples were drawn using NUTS with a diagonal Euclidean metric.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
\end{CodeOutput}
\end{CodeChunk}
%
The estimated potential scale reduction statistic $\hat{R}$
(\code{Rhat}) value of 1.0 for the parameter of interest \code{theta}
is consistent with convergence.  \proglang{Stan} uses a more
conservative version of $\hat{R}$ than is usual in packages such as
\pkg{Coda}, first splitting each chain in half to diagnose
montonically increasing or decreasing samples; \citep{GelmanEtAl:2013}
and the reference manual provide detailed definitions.

The estimated number of effective samples per parameter
(\code{n\_eff}) is more than 2000, far more than we actually need for
inference.  As with $\hat{R}$, the number of effective samples is
estimated conservatively using both cross-chain and within-chain
estimates; \citep{GelmanEtAl:2013} and the reference manual provide
details.

The posterior standard deviation (\code{sd}), median (\code{50\%}) and
95\% posterior interval (\code{2.5\%},\code{97.5\%}) are also shown;
the actual print command also prints the 50\% interval boundaries, but
these are ellided for space in the above output.  The MCMC standard
error (\code{se}), defined to be the posterior standard deviation
divided by the square root of the number of effective samples
(\code{sd / sqrt(n\_eff)}), is small relative to the the posterior
standard deviation.


\section[Stan models]{\proglang{Stan} models}

In the rest of this paper, we will concentrate on the modeling
language itself and how it is executed.  For more details on
Hamiltonian Monte Carlo, see \citep{Neal:2011}; for more information
on the no-U-turn sampler used by default in \proglang{Stan}, see
\citep{Hoffman-Gelman:2011}.

\subsection{Example: hierarchical model, with inference}

\cite[Section 5.1]{GelmanEtAl:2013} define a hierarchical model of the
incidence of tumors in rats in control groups across trials; a very
similiar model is defined for mortality rates in pediatric surgeries
across hospitals in \citep[Examples, Volume 1]{LunnEtAl:2000}.%  


\begin{figure}
\begin{Code}
data {
  int<lower=0> J;
  int<lower=0> y[J];
  int<lower=0> n[J];
}
parameters {
  real<lower=0,upper=1> theta[J];
  real<lower=0,upper=1> lambda;
  real<lower=0.1> kappa;
}
transformed parameters {
  real<lower=0> alpha;
  real<lower=0> beta;
  alpha <- lambda * kappa;
  beta <- (1 - lambda) * kappa;
}
model {
  lambda ~ uniform(0,1);     // priors
  kappa ~ pareto(0.1,1.5);
  theta ~ beta(alpha,beta);
  y ~ binomial(n,theta);     // likelihood
}
generated quantities {
  real<lower=0,upper=1> avg;
  int<lower=0,upper=1> above_avg[J];
  int<lower=1,upper=J> rnk[J];
  avg <- mean(theta);
  for (j in 1:J)
    above_avg[j] <- (theta[j] > avg);
  for (j in 1:J)
    rnk[j] <- rank(theta,j) + 1;
}
\end{Code}
  \caption{Hierarchical binomial model with posterior inferences,
    coded in \proglang{Stan}.}
\end{figure}
%
A \proglang{Stan} implementation is provided in
Figure~\ref{hier-binom.fig}.  In the rest of this section, we will
walk through what the meaning of the various blocks are for the
execution of the model.

\subsection{Data block}

A \proglang{Stan} program starts with an (optional) data block, which
declares the data required to fit the model.  In the model in
Figure~\ref{hier-binom.fig}, the data block declares an integer
variable \code{J} for the number of groups in the hierarchical model.
The arrays \code{y} and \code{n} have size \code{J}, with \code{y[j]}
being the number of positive outcomes in \code{n[j]} trials.  

All of these variables are declared with a lower-bound constraint
restricting their values to be greater than or equal to zero.
\proglang{Stan}'s constraint language is not strong enough to restrict
each \code{y[j]} to be less than or equal to \code{n[j]}.

The data for a \proglang{Stan} model is read in once as the
\proglang{C++} object representing the model is constructed.  After
the data is read in, the constraints are validated.  Failure to
validate data will terminate the program and print an error message.

\subsection{Transformed data block}

The model in Figure~\ref{hier-binom.fig} does not have a transformed
data block.  A transformed data block may be used to define new
variables that can be computed based on the data.  For example,
standardized versions of data can be defined in a transformed
data block or Bernoulli trials can be summed to model as binomial.
Any constant data can also be defined in the transformed data block.

The transformed data block starts with a sequence of variable
declarations and continues with a squence of statements defining the
variables.  For example, the following transformed data block declares
a vector \code{x\_std}, then defines it to be the standardization of \code{x}.
%
\begin{Code}
transformed data {
  vector[N] x_std;
  x_std <- (x - mean(x)) / sd(x);
}
\end{Code}

The transformed data block is executed during construction, after the
data is read in.  Any data variables declared in the data block may be
used in the variable declarations or statements.  Transformed data
variables may be used after they are declared, although care must be
taken to ensure they are defined before they are used.  Any
constraints declared on transformed data variables are validated after
all of the statements are executed, with execution terminating with an
error message at the first variable with an invalid value.

\subsection{Parameter block}

The parameter block in the program in Figure~\ref{hier-binom.fig}
defines three parameters.  The parameter \code{theta[j]} represents
the probability of success in group \code{j}.  The prior on each
\code{theta[j]} is parameterized by a mean \code{lambda} and prior
count \code{kappa}.  Both \code{theta[j]} and \code{lambda} are
constrained to fall between zero and one, whereas \code{lambda} is
constrained to be greater than or equal to 0.1 to match the support of
the Pareto prior it receives in the model block.

The parameter block is executed every time the log probability is
evaluated.  This may be multiple times per iteration of a sampling or
optimization algorithm.  Furthermore, different samplers and
optimizers use different complexities of calls to the log probability
function.  HMC and NUTS require gradients and Riemann-Manifold HMC
requires up to third-order derivatives, whereas Metropolis-Hastings
sampling requires no derivatives.  Various gradients and higher-order
derivatives can also be used in conjugate gradient optimization or
quasi-Newton methods.

The probability distribution defined by a \proglang{Stan} program is
intended to have unconstrained support (i.e., no points of zero
probabilty).  Unconstrained support greatly simplifies the task of
writing samplers or optimizers.  To achieve unbounded support,
variables declared with constrained support are transformed.  For
instance, variables declared on $[0,1]$ are log-odds transformed and
non-negative variables declared to fall in $[0,\infty)$ are log
transformed.  More complex transforms are required for simplexes (a
reverse stick-breaking transform) and covariance matrices (Cholesky
factorization).  The dimensionality of the resulting probabilty
function may change as a result of the transform. For example, a $K
\times K$ covariance matrix requires only ${K \choose 2} + K$
unconstrained parameters, and a $K$-simplex requires only $K-1$
unconstrained parameters.

The unconstrained paramters over which the model is defined are
inverse transformed back to their constrained forms before executing
the model code.  To account for the change of variables, the log
absolute Jacobian determinant of the inverse transform is added to the
overall log probabilty function.%
%
\footnote{For optimization, the Jacobian adjustment may be optionally
  suppressed to guarantee the optimizer finds the maximum of the log
  probability function on the constrained parameters.}
%
The gradients of the log probabilty function exposed include the
Jacobian term.  

There is no validation required for the parameter block because the
variable transforms are guaranteed to produce values that satisfy the
declared constraints.


\subsection{Transformed parameters block}

The transformed parameters block allows user to define transforms of
parameters within a model.  Following the model in
\citep{GelmanEtAl:2013}, the example in Figure~\ref{hier-binom.fig}
uses the transformed parameter block to define transformed parameters
\code{alpha} and \code{beta} for the prior success and failure counts
to use in the beta prior for \code{theta}.  

Following the same convention as the transformed data block, the
(optional) transformed parameter block begins with declarations of the
transformed parameters, followed by a sequence of statements defining
them.  Variables from previous blocks as well as the transformed
parameters block may be used.  In the example, the prior success and
failure counts \code{alpha} and \code{beta} are defined in terms of
the prior mean \code{lambda} and total prior count \code{kappa}.

The transformed parameter block is executed after the parameter
block.  Constraints are validated after all of the statements defining the
transformed parameters have executed.  Failure to validate a
constraint results in an exception being thrown, which halts the
execution of the log probability function.  The log probability
function can be defined to return negative infinity or a special
not-a-number value. 

Values of transformed parameters are saved in the output along
with the parameters.  As an alternative, local variables can be used
to define temporary values that do not need to be saved.  

\subsection{Model block}

The model block defines the log probability function.  The example in
Figure~\ref{hier-binom.fig} has a simple model containing four
sampling statements.  The hyperprior on the prior mean \code{lambda}
is uniform, and the hyperprior on the prior count \code{kappa} is a
Pareto distribution with lower-bound of support at 0.1 and shape 1.5,
leading to a probability of $\kappa > 0.1$ proportional to
$\kappa^{-5/2}$.  The hierarchical prior on \code{theta} is
vectorized.  The elements of \code{theta} are drawn independently from
a beta distribution with prior success count \code{alpha} and prior
failure count \code{beta}.  Both \code{alpha} and \code{beta} are
transformed parameters, but because they are only used on the
right-hand side of a sampling statement do not require a Jacobian
adjustment of their own.  The likelihood function is also vectorized,
with the effect that each success count \code{y[i]} is drawn from a
binomial distribution with number of trials \code{n[i]} and chance of
success \code{theta[i]}.  In vectorized sampling statements, single
values may be repeated as many times as necessary.  

The model block is executed after the transformed parameters block
every time the log probability function is evaluated.  

\subsection{Generated quantities block}

The (optional) generated quantities allows values that depend on
parameters and data, but do not affect estimation, to be defined
efficiently.  The generated quantities block is called only once per
sample, not once per log probability function evaluation.  It may be
used to calculate predictive inferences as well as to carry out
forward simulation for predictive posterior checks.

The \proglang{BUGS} surgical example explored the ranking of
institutions in terms of surgical mortality.  This is coded in the
example in Figure~\ref{hier-binom.fig} using the generated quantities
block.  The generated quantity variable \code{rnk[j]} will hold the
rank of institution \code{j} from 1 to \code{J} in terms of mortality
rate \code{theta[j]}.  The ranks are extracted using the \code{rank}
function. The posterior summary will print average rank and deviation,
but the values can also be extracted in order to plot posterior rank
histograms by institution as done in \citep[Examples, Volume
1]{LunnEtAl:2000}.

As a second illustration, we calculate the (posterior) probability
that a given institution is above-average in terms of mortality rate.
This is done for each institution \code{j} with the usual plug-in
estimate of \code{theta[j] > mean(theta)}, which returns a binary (0
or 1) value.  The posterior mean of \code{above\_avg[j]} calculates
the posterior probability $\mbox{Pr}[\theta_j > \bar{\theta}|y,n]$
according to the model.

\subsection{Initialization}

\proglang{Stan}'s samplers and optimizers all start from either random
or user-supplied initial values for the parameters.  If the user
supplies initial values, these must be validated and transformed to
the unconstrained space.  If a variable's value does not satisfy its
declared constraints, the program exits and an error message is
printed.  If initial values are supplied randomly, then they must be
generated randomly.  The default initialization is to randomly
generate values uniformly on $[-2,2]$, which supplies fairly diffuse
starting points when transformed back to the constrained scale.

\subsection{Variable definition and block execution summary}

A table summarizing the point at which variables are read, written,
or defined is provided in Figure~\ref{block-actions.fig}. 
%
\begin{figure}
\begin{center}
\begin{tabular}{l|c|l|l}
{\it block} & {\it statements?} & {\it action} & {\it evaluated}
\\\hline\hline
\code{user initialization} & n/a & transform & chain
\\[3pt]
\code{random initialization} & n/a & randomize & chain 
\\\hline\hline
\code{data} & no & read & chain  
\\
\code{transformed data} & yes & evaluate & chain  
\\ \hline
\code{parameters} & no & inv.\ transform, Jacobian & leapfrog  \\
& & inv.\ transform, write & sample 
\\[3pt]
\code{transformed parameters} & yes & evaluate & leapfrog \\
& & write & sample 
\\\hline
\code{model} & yes & evaluate & leapfrog
\\\hline
\code{generated quantities} & yes & evaluate & sample \\
& & write & sample
\end{tabular}
\end{center}
\caption{The read, write, transform, and evaluate actions and
  periodicities listed in the last column correspond to the
  \proglang{Stan} program blocks in the first column.  The middle
  column indicates whether the block allows statements.  The last row
  indicates that parameter initialization requires a read and
  transform operation applied once per chain.}%
\label{block-actions.fig}
\end{figure}
%
This table is defined assuming HMC or NUTS samplers, which require a
log probability and gradient calculation for one or more leapfrog
steps per iteration.  

%
\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it variable kind} & {\it declaration block}
\\ \hline\hline
% constants & \code{transformed data}
% \\ \hline
unmodeled data & \code{data}, \code{transformed data}
\\ 
modeled data & \code{data}, \code{transformed data}
\\ \hline
missing data & \code{parameters}, \code{transformed parameters}
\\
modeled parameters & \code{parameters}, \code{transformed parameters}
\\
unmodeled parameters & \code{data}, \code{transformed data}
\\[2pt] \hline
generated quantities & \code{transformed data}, \code{transformed parameters}, 
\\ 
& \code{generated quantities}
\\ \hline\hline
loop indices & any loop statement
\\ \hline
local variables & any statement block
\\ 
\end{tabular}
\end{center}
\caption{Variables of the kind indicated in the left column must
 be declared in one of the blocks declared in the right
 column.}\label{variable-kinds.fig}
\end{figure}
%
\cite[p.~366]{GelmanHill:2007} provide a taxonomy of the kinds of
variables used in Bayesian models.  Figure~\ref{variable-kinds.fig}
contains Gelman and Hill's taxonomy aligned with the corresponding
locations of declarations and definitions in \proglang{Stan}.
Unmodeled data variables includes size constants and regression
predictors.  Modeled data variables include known outcomes or
measurements.  A data variable or constant literal is modeled in a
\proglang{Stan} program if it (or a variable that depends on it)
occurs on the left-hand side of a sampling statement.  Unmodeled
parameters are provided as data, and are either known or fixed to some
value for convenience, a typical use being the parameters of a weakly
informative prior for a parameter.

A modeled parameter is given a distribution by the model (usually
dependent on data and correlated with other parameters).  This can be
done either by placing it (or a variable that depends on it) on the
left-hand side of a sampling statement or by declaring the variable
with a constraint (see Section~\label{implicit-prior.section}).

Any variable that occurs in a (transformed) data block can also be
provided instead as a constant.  So a user may decide to write \code{y
  ~ normal(0,1)} or to declare \code{mu} and \code{sigma} as data, and
write \code{y ~ normal(mu,sigma)}.  The latter choice allows the
parameters to be changed without recompiling the model, but requires
them to be specified as part of the input data.

Missing data is a new variable type included here.  In order to
perform inference on missing data, it must be declared as a parameter
and modeled.  

Generated quantities include simple transforms of data required for
printing.  For example, the variable of interest might be a standard
deviation, resulting from transforming a precision parameter $\tau$ to
$\tau^{-1/2}$.  A non-linearly transformed variable will have
different effective sample sizes and $\hat{R}$ statistics than the
variable it is derived from, so it is convenient to define them in the
generated quantities block.  As seen in the example in
Figure~\ref{hier-binom.fig}, generated quantities may also be used for
event probability estimates.  

The generated quantities block may also be used for forward
simulations, generating values to make predictions or to perform
posterior predictive checks.  The generated quantities block is the
only location in \proglang{Stan} in which random-number generators may
be applied explicitly (they are implicit in parameters).

Calculations in the generated quantities block do not impact the
estimates of the parameters.  In this way, it can be used like the cut
feature of \proglang{BUGS}.  Nevertheless, we recommend full Bayesian
inference in general, and do not use it for cut-like behavior unless
it would have no impact (as in generated predictions for unseen data
in many models).

The list is completed with two types of local variables, loop indices
and traditional local variables.  Unlike \proglang{BUGS},
\proglang{Stan} allows local variables to be declared and assigned.
For example, it is possible to compute the sum of the squares of
entries in an array or vector \code{y} as follows.
%
\begin{Code}
{ 
  real sum_of_squares;
  sum <- 0;
  for (n in 1:N)
    sum <- sum + y[n] * y[n];
}  
\end{Code}
%
Local variables may not be declared with constraints, because there is
no location at which it makes sense to test that they satisfy the
constraints. 


\subsection{Implicit Uniform Priors}\label{implicit-prior.section}

The default distribution for a variable is uniform over its support.
For instance, a variable declared with a lower bound of 0 and an upper
bound of 1 implicitly receives a $\mbox{\sf Uniform}(0,1)$
distribution.  These implicit uniform priors are improper if the
variable has unbounded support.  For instance, the uniform
distribution over simplexes and correlation matrices is proper, but
the uniform distribution over ordered vectors or covariance matrices
is not. 


\section{Data types}

Expressions in \proglang{Stan}, including variables are statically
typed.  This means their type is declared at compile time as part of
the model, and does not change throughout the execution of the
program.  This is the same behavior as is found in compiled
programming languages such as \proglang{C(++)}, \proglang{Fortran},
and \proglang{Java}, but is unlike the behavior of interpreted
languages such as \proglang{BUGS}, \proglang{R}, and
\proglang{Python}.  We believe that statically typing the variables
(as well as declaring them in appropriate blocks based on usage),
makes \proglang{Stan} programs easier to read and easier to debug by
making the modeling decisions and expression types explicit.

\subsection{Primitive Types}

The primitive types of \proglang{Stan} are \code{real} and \code{int},
which are used to represent continuous and integral values.  Integer
expressions can be used anywhere a real value is required, but not
{\it vice-versa}.

\subsection{Vector and Matrix Types}

\proglang{Stan} supports vectors, row vectors, and matrices with the
usual access operations.  Indexing for vector, matrix, and array types
starts from one.  

Vectors are declared with their sizes and matrices with their number
of rows and columns.

All vector and matrix types contain real values and may not be
declared to contain integers.  Sequences of integers are represented
using arrays.

\subsection{Array Types}

An array may have entries of any other type.  For example, arrays of
integers and reals are allowed, as are arrays of vectors or arrays of
matrices.  

Higher-dimensional arrays are intrinsically arrays of arrays.  An
entry two-dimensional array \code{y} may be accessed as \code{y[1,2]}
or \code{y[1][2]}, with the expression \code{y[1]} denoting an array.%
%
\footnote{ Arrays are stored in row-major order and matrices in
  column-major order.}

\subsection{Constrained Variable Types}

Variables may be declared with constraints, which have different
effects depending on where the variable is declared.

Integer and real types may be provided with lower bounds, upper
bounds, or both.  This includes the types used in arrays, and the real
types used in vectors and matrices.

Vector types may be constrained to be unit simplexes (all entries
non-negative and summing to 1), unit length (sum of squares is 1), or
ordered (entries are in ascending order), positive ordered (entries in
ascending order, all non-negative), using the types \code{simplex[K]},
\code{unit_vector[K]}, \code{ordered[K]}, or
\code{positive_ordered[K]}, where \code{K} is the size of the vector.

Matrices may be constrained to be covariance matrices (symmetric,
positive definite) or correlation matrices (symmetric, positive
definite, unit diagonal), using the types \code{cov_matrix[K]} and
\code{corr_matrix[K]}.

\section{Expressions and Type Inference}

The syntax of \code{Stan} is defined in terms of expressions, which
denote values of a particular type, and statements, which represent
control, assignment and sampling.

\subsection{Expressions}

\proglang{Stan} provides the usual kinds of expressions found in
programming languages.  This includes variables, literals denoting
integers, real values or strings, binary and unary operators over
expressions, and function application.  

\subsubsection{Type Inference}

The type of each variable is declared and cannot change.  

The type of a numeric literal is determined by whether or not it
contains a period or scientific notation; for example, \code{20} has
type \code{int} whereas \code{20.0} and \code{2e+1} have type
\code{real}.

The type of applying an operator or a function to one or more
expressions is determined by the available signatures for the
function.  For example, the multiplication operator (\code{*}) has a
signature that maps two \code{int} arguments to a \code{real} and two
\code{real} arguments to a \code{real} result.  Another signature maps
a \code{row\_vector} and a \code{vector} to a \code{real} result.

\subsubsection{Type Promotion}

If necessary, an integer type will be promoted to a \code{real} value.
For example, multipling an \code{int} by a \code{real} produces a
\code{real} result by promoting the \code{int} argument to a
\code{real}.  \proglang{Stan} also attempts to treat arrays as
covariant, meaning that arrays of of \code{int} values can be used
where arrays of \code{real} values are specified.

\section{Statements}

\subsection{Assignment and Sampling}

\proglang{Stan} supports the same two basic statements as
\proglang{BUGS}, assignment and sampling, examples of which we
introduced earlier.  Unlike in \proglang{BUGS}, where these two
flavors of statement define a directed acyclic graphical model, in
\proglang{Stan}, they define a log probability function.  There is an
implicitly defined variable \code{lp\_\_} denoting the log probability
that will be returned by the log probability function, and a sampling
statement is nothing more than shorthand for incrementing it.  For
example, the sampling statement
%
\begin{Code}
y ~ normal(0,1);
\end{Code}
%
has the exact same effect as the statement
%
\begin{Code}
lp__ <- lp__ + normal_log(y,0,1);
\end{Code}

\subsection{Sequences of statements and execution order}

\proglang{Stan} allows sequences of statements wherever statements may
occur. Unlike \proglang{BUGS}, in which statements define a directed
acyclic graph, in \proglang{Stan}, statements are executed
imperatively in the order in which they occur in a program.

\subsubsection{Blocks}

Sequences of statements surrounded by curly braces (\code{\{}) and
\code{\}}) form blocks.  Blocks may start with local variable
declarations.  The scope of a local variable (i.e., where it is
available to be used) is that of the block in which it is declared;
other variables, such as those declared as data or parameters, have
scope continuing until the end of the program but may only be assigned
in the block in which they are declared.


\subsection{Whitespace, semicolons, and comments}

Following the convention of \proglang{C++}, statements are separated
with semicolons in \proglang{Stan} so that the content of whitespace
(outside of comments) is irrelevant.  This is in contrast to
\proglang{BUGS} and \proglang{JAGS} in which carriage returns are
special and can indicate the end of a statement.

\proglang{Stan} supports the line comment style of \proglang{C++},
using two forward slashes (\code{//}) to comment out the rest of a
line; this is the one location where whitespace type matters.  It also
supports the line comment style of \proglang{R} and \proglang{BUGS},
treating a pound sign (\code{\#}) as commenting out everything until
the end of the line.  \proglang{Stan} also supports
\proglang{C++}-style block comments, with everything between the
start-comment (\code{/*}) and end-comment (\code{*/}) markers being
ignored.


and \proglang{R}, with the \proglang{C++} style being preferred.  

\subsection{Control structures}

\proglang{Stan} supports the same kind of explicitly bounded for loops
as found in \code{BUGS} and \code{R}.  Like \code{R}, but unlike
\proglang{BUGS}, \proglang{Stan} supports while loops and conditional
(if-then-else) statements.%
%
\footnote{\proglang{BUGS} omits these control structures because they
  are not useful for defining directed, acyclic graphs.}
%
\proglang{Stan} provides the usual comparison operators and boolean
operators to help define conditionals and condition-controlled while
loops.  

\subsection{Print statements and debugging}

\proglang{Stan} provides print statements which take arbitrarily many
arguments consisting of expressions or string literals consisting of
sequences of characters surrounded by double quotes (\code{"}).
These statements may be used for debugging purposes to report on
intermediate states of variables or to indicate how far execution has
proceeded before an error.

\section[The Stan library]{The \proglang{Stan} library}







%\bibliographystyle{jss}
\bibliography{stan-paper}

\end{document}

